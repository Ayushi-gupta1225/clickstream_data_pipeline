app{
  name="ClickstreamDataPipeline"
  version="1.0"
}

input{
  path1="C:/Target/ClickstreamProject/src/test/scala/data_in/clickstream_log (1).csv"
  path2="C:/Target/ClickstreamProject/src/test/scala/data_in/item_data (1).csv"
}

output{
  path="C:/Target/ClickstreamProject/src/test/scala/data_out/clickstream_data_events"
  clickstreamnullpath="C:/Target/ClickstreamProject/src/test/scala/data_out/clickstreamnulls"
  itemsetnullpath="C:/Target/ClickstreamProject/src/test/scala/data_out/itemsetnulls"
  clickstreamduplicatespath="C:/Target/ClickstreamProject/src/test/scala/data_out/clickstreamduplicates"
  itemsetduplicatespath="C:/Target/ClickstreamProject/src/test/scala/data_out/itemsetduplicates"
}

spark{
  master="local[*]"
  appName=${app.name}
  logLevel="ERROR"
  //spark.executor.memory="2g"
  //spark.default.parallelism=4
  //spark.sql.shuffle.partitions=10
  //spark.streaming.backpressure.enabled=true
  //spark.streaming.kafka.maxRatePerPartition=1000
}